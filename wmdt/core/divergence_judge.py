#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DivergenceJudge: åˆ†æ­§è¯„ä¼°å™¨

é€šè¿‡ LLM è¯„ä¼°ä¸¤ä¸ª BeliefState ä¹‹é—´çš„è®¤çŸ¥åˆ†æ­§ç¨‹åº¦
"""

import json
import re
from typing import Dict, List, Tuple, Optional
from pydantic import BaseModel, Field

from metagpt.llm import LLM
from metagpt.logs import logger

from wmdt.core.belief_state import BeliefState, BeliefTrajectory
from wmdt.core.prompts import DIVERGENCE_JUDGE_PROMPT


class DivergenceScore(BaseModel):
    """åˆ†æ­§è¯„åˆ†ç»“æœ"""
    step: int = Field(description="æ­¥éª¤ç¼–å·")
    role_name: str = Field(description="è§’è‰²åç§°")

    global_divergence: float = Field(ge=0.0, le=1.0, description="å…¨å±€çŠ¶æ€åˆ†æ­§")
    goal_divergence: float = Field(ge=0.0, le=1.0, description="ç›®æ ‡åˆ†æ­§")
    teammate_divergence: float = Field(ge=0.0, le=1.0, description="é˜Ÿå‹æ¨¡å‹åˆ†æ­§")
    risk_divergence: float = Field(ge=0.0, le=1.0, description="é£é™©è¯†åˆ«åˆ†æ­§ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰")
    overall_divergence: float = Field(ge=0.0, le=1.0, description="æ€»ä½“åˆ†æ­§")

    explanation: str = Field(description="åˆ†æ­§è§£é‡Š")

    def is_high_divergence(self, threshold: float = 0.5) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜åˆ†æ­§"""
        return self.overall_divergence >= threshold

    def is_risk_spike(self, threshold: float = 0.6) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé£é™©åˆ†æ­§é£™å‡ï¼ˆPCI çš„æ½œåœ¨ä¿¡å·ï¼‰"""
        return self.risk_divergence >= threshold


class DivergenceJudge:
    """
    åˆ†æ­§è¯„ä¼°å™¨

    å¯¹æ¯”å‚è€ƒè½¨è¿¹å’Œå¤±è´¥è½¨è¿¹ï¼Œé€æ­¥è¯„ä¼° BeliefState çš„åˆ†æ­§ï¼Œ
    æ‰¾åˆ° risk_divergence é£™å‡çš„ç‚¹ï¼ˆPCI å€™é€‰ç‚¹ï¼‰
    """

    def __init__(self, llm: Optional[LLM] = None):
        self.llm = llm or LLM()
        self.divergence_scores: List[DivergenceScore] = []

    async def evaluate_divergence(
        self,
        reference_belief: BeliefState,
        failed_belief: BeliefState
    ) -> Optional[DivergenceScore]:
        """
        è¯„ä¼°ä¸¤ä¸ª BeliefState ä¹‹é—´çš„åˆ†æ­§

        Args:
            reference_belief: å‚è€ƒè½¨è¿¹çš„ BeliefStateï¼ˆæˆåŠŸè·¯å¾„ï¼‰
            failed_belief: å¤±è´¥è½¨è¿¹çš„ BeliefState

        Returns:
            DivergenceScore å¯¹è±¡ï¼ŒåŒ…å«å„ç»´åº¦çš„åˆ†æ­§è¯„åˆ†
        """
        try:
            # æ„é€ è¯„ä¼° Prompt
            prompt = DIVERGENCE_JUDGE_PROMPT.format(
                reference_belief=reference_belief.to_json(indent=2),
                failed_belief=failed_belief.to_json(indent=2)
            )

            logger.info(
                f"Evaluating divergence: {reference_belief.role_name} "
                f"Step {reference_belief.step}"
            )

            # è°ƒç”¨ LLM è¯„ä¼°
            response = await self.llm.aask(prompt)

            # è§£æ JSON
            score_data = self._parse_divergence_json(response)
            if not score_data:
                logger.warning("Failed to parse divergence score JSON")
                return None

            # æ„é€  DivergenceScore
            score = DivergenceScore(
                step=reference_belief.step,
                role_name=reference_belief.role_name,
                global_divergence=score_data.get("global_divergence", 0.0),
                goal_divergence=score_data.get("goal_divergence", 0.0),
                teammate_divergence=score_data.get("teammate_divergence", 0.0),
                risk_divergence=score_data.get("risk_divergence", 0.0),
                overall_divergence=score_data.get("overall_divergence", 0.0),
                explanation=score_data.get("explanation", "")
            )

            logger.info(
                f"Divergence: overall={score.overall_divergence:.2f}, "
                f"risk={score.risk_divergence:.2f}"
            )

            self.divergence_scores.append(score)
            return score

        except Exception as e:
            logger.error(f"Error evaluating divergence: {e}")
            return None

    def _parse_divergence_json(self, response: str) -> Optional[dict]:
        """è§£æ LLM è¿”å›çš„ JSONï¼ˆå¤ç”¨ observable_role çš„é€»è¾‘ï¼‰"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON ä»£ç å—
            json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            match = re.search(json_pattern, response, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(1))
                except json.JSONDecodeError:
                    pass

            # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
            json_obj_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            match = re.search(json_obj_pattern, response, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(0))
                except json.JSONDecodeError:
                    pass

            return None

    async def evaluate_trajectories(
        self,
        reference_trajectory: BeliefTrajectory,
        failed_trajectory: BeliefTrajectory
    ) -> List[DivergenceScore]:
        """
        è¯„ä¼°ä¸¤æ¡å®Œæ•´è½¨è¿¹çš„åˆ†æ­§

        æŒ‰æ­¥éª¤å’Œè§’è‰²å¯¹é½ï¼Œé€å¯¹è¯„ä¼° BeliefState

        Returns:
            æ‰€æœ‰è¯„ä¼°çš„ DivergenceScore åˆ—è¡¨
        """
        logger.info(
            f"Evaluating trajectories: "
            f"Reference={len(reference_trajectory.belief_states)} states, "
            f"Failed={len(failed_trajectory.belief_states)} states"
        )

        # æŒ‰ (step, role_name) å¯¹é½ä¸¤æ¡è½¨è¿¹
        aligned_pairs = self._align_trajectories(
            reference_trajectory.belief_states,
            failed_trajectory.belief_states
        )

        scores = []
        for ref_belief, failed_belief in aligned_pairs:
            score = await self.evaluate_divergence(ref_belief, failed_belief)
            if score:
                scores.append(score)

        return scores

    def _align_trajectories(
        self,
        ref_beliefs: List[BeliefState],
        failed_beliefs: List[BeliefState]
    ) -> List[Tuple[BeliefState, BeliefState]]:
        """
        å¯¹é½ä¸¤æ¡è½¨è¿¹çš„ BeliefState

        æŒ‰ (step, role_name) é…å¯¹

        Returns:
            [(å‚è€ƒbelief, å¤±è´¥belief), ...] åˆ—è¡¨
        """
        pairs = []

        # åˆ›å»ºç´¢å¼•ï¼š(step, role_name) -> BeliefState
        ref_index = {(b.step, b.role_name): b for b in ref_beliefs}
        failed_index = {(b.step, b.role_name): b for b in failed_beliefs}

        # æ‰¾åˆ°å…¬å…±çš„ (step, role_name) é”®
        common_keys = set(ref_index.keys()) & set(failed_index.keys())

        for key in sorted(common_keys):
            pairs.append((ref_index[key], failed_index[key]))

        logger.info(f"Aligned {len(pairs)} BeliefState pairs")
        return pairs

    def find_pci(self, risk_threshold: float = 0.6) -> Optional[DivergenceScore]:
        """
        æŸ¥æ‰¾ PCI (Point of Causal Inevitability)

        å®šä¹‰ï¼šrisk_divergence é¦–æ¬¡æ˜¾è‘—é£™å‡çš„ç‚¹

        Args:
            risk_threshold: risk_divergence é˜ˆå€¼

        Returns:
            PCI å¯¹åº”çš„ DivergenceScoreï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        for score in self.divergence_scores:
            if score.is_risk_spike(risk_threshold):
                logger.info(
                    f"ğŸ¯ PCI Found: Step {score.step}, Role {score.role_name}, "
                    f"risk_divergence={score.risk_divergence:.2f}"
                )
                return score

        logger.warning("No PCI found (no risk_divergence spike detected)")
        return None

    def save_results(self, filepath: str):
        """ä¿å­˜åˆ†æ­§è¯„ä¼°ç»“æœ"""
        data = {
            "divergence_scores": [score.model_dump() for score in self.divergence_scores],
            "pci": self.find_pci().model_dump() if self.find_pci() else None
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved {len(self.divergence_scores)} divergence scores to {filepath}")
